# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TtN8LHhLJmzgUy6LdtPOCuhecAp3nzJF
"""





from pdb import set_trace
import mne
import pandas as pd
import numpy as np
import math
import os
import h5py
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN,LSTM, Dense, Activation, Bidirectional
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Convolution2D
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import CSVLogger
import random


def overlap(range1,range2):
    if (range1[0] >= range2[0] and range1[0]<=range2[1]) or (range1[1] >= range2[0] and range1[1]<=range2[1]):
        return True
    else:
        return False


def my_read(path,path1,typee):
    filesedf=[]
    f_edf=pd.DataFrame()
    sp=pd.DataFrame()
    eeg_data = []
    first_d = True
    for file in os.listdir(path):
        if '.edf' in file:
            f=os.path.join(path,file)

            edf_file = mne.io.read_raw_edf(f,  eog = ['FP1', 'FP2', 'F3', 'F4',
       			'C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'F7', 'F8',
       			'T3', 'T4', 'T5', 'T6', 'PZ', 'FZ', 'CZ', 'A1', 'A2'
       			], verbose = 'error', preload = True)
            #print('hooo')
            edf_file_down_sampled = edf_file.resample(250, npad = "auto")# set sampling frequency to 250 Hz
            ed = edf_file_down_sampled.to_data_frame(picks = None, index = None, scalings = 100000,copy = True, start = None, stop = None)# converting into dataframe
            Fp1_Fp7 = (ed.loc[: , 'FP1']) - (ed.loc[: , 'F7'])
            FP2_F8 = (ed.loc[: , 'FP2']) - (ed.loc[: , 'F8'])
            F7_T3 = (ed.loc[: , 'F7']) - (ed.loc[: , 'T3'])
            F8_T4 = (ed.loc[: , 'F8']) - (ed.loc[: , 'T4'])
            T3_T5 = (ed.loc[: , 'T3']) - (ed.loc[: , 'T5'])
            T4_T6 = (ed.loc[: , 'T4']) - (ed.loc[: , 'T6'])
            T5_O1 = (ed.loc[: , 'T5']) - (ed.loc[: , 'O1'])
            T6_O2 = (ed.loc[: , 'T6']) - (ed.loc[: , 'O2'])
            A1_T3 = (ed.loc[: , 'A1']) - (ed.loc[: , 'T3'])
            T4_A2 = (ed.loc[: , 'T4']) - (ed.loc[: , 'A2'])
            T3_C3 = (ed.loc[: , 'T3']) - (ed.loc[: , 'C3'])
            C4_T4 = (ed.loc[: , 'C4']) - (ed.loc[: , 'T4'])
            C3_CZ = (ed.loc[: , 'C3']) - (ed.loc[: , 'CZ'])
            CZ_C4 = (ed.loc[: , 'CZ']) - (ed.loc[: , 'C4'])
            FP1_F3 = (ed.loc[: , 'FP1']) - (ed.loc[: , 'F3'])
            FP2_F4 = (ed.loc[: , 'FP2']) - (ed.loc[: , 'F4'])
            F3_C3 = (ed.loc[: , 'F3']) - (ed.loc[: , 'C3'])
            F4_C4 = (ed.loc[: , 'F4']) - (ed.loc[: , 'C4'])
            C3_P3 = (ed.loc[: , 'C3']) - (ed.loc[: , 'P3'])
            C4_P4 = (ed.loc[: , 'C4']) - (ed.loc[: , 'P4'])
            P3_O1 = (ed.loc[: , 'P3']) - (ed.loc[: , 'O1'])
            P4_O2 = (ed.loc[: , 'P4']) - (ed.loc[: , 'O2'])
            data = {'Fp1_Fp7': Fp1_Fp7, 'FP2_F8': FP2_F8,	'F7_T3': F7_T3,'F8_T4': F8_T4,'T3_T5': T3_T5,'T4_T6': T4_T6,	'T5_O1': T5_O1,	'T6_O2': T6_O2,	'A1_T3': A1_T3,	'T4_A2': T4_A2,	'T3_C3': T3_C3,	'C4_T4': C4_T4,'C3_CZ': C3_CZ,'CZ_C4': CZ_C4,'FP1_F3': FP1_F3,	'FP2_F4': FP2_F4,	'F3_C3': F3_C3,	'F4_C4': F4_C4,	'C3_P3': C3_P3,	'C4_P4': C4_P4,	'P3_O1': P3_O1,	'P4_O2': P4_O2}
            new_data_frame = pd.DataFrame(data, columns = ['Fp1_Fp7', 'FP2_F8', 'F7_T3', 'F8_T4', 'T3_T5', 'T4_T6', 'T5_O1', 'T6_O2', 'A1_T3', 'T4_A2', 'T3_C3', 'C4_T4', 'C3_CZ',	'CZ_C4', 'FP1_F3', 'FP2_F4', 'F3_C3', 'F4_C4', 'C3_P3', 'C4_P4', 'P3_O1', 'P4_O2'])
            fs = edf_file_down_sampled.info['sfreq']
            [row, col] = new_data_frame.shape
            n = math.ceil(row / (750 - (fs /5)))
            csvfile = str(int(file[:-4])) + '.csv'
            f2=os.path.join(path1,csvfile)

            if os.path.isfile(f2):
                 ann=pd.read_csv(f2)

                 cols=['Start time','End time', 'File Start']
                 annotation = ann[cols]
                 t=['strt_time','end_time','file_strt']
                 datat=pd.DataFrame()
                 datat[t]=pd.DataFrame(annotation)
                 dataf=datat.iloc[0:1,2]
                 dataff=pd.DataFrame()
                 dataff=pd.DataFrame(dataf)
                 ff= datat.iloc[0,2]
                 st=datat.strt_time.str.split(":",expand=True)
                 et=datat.end_time.str.split(":",expand=True)
                 f_strt=dataff.file_strt.str.split(":",expand=True)
                 k=st.astype('int')
                 j=et.astype('int')
                 f=f_strt.astype('int')

                 s_time=(k[0]*3600 + k[1]*60+k[2] +k[3]/1000) # conversion into seconds
                 e_time=(j[0]*3600 + j[1]*60+j[2] + j[3]/1000) # conversion into seconds
                 f_time=(f[0]*3600 + f[1]*60+f[2])
                 xx=f_time.at[0] # getting single value of file strt
                 fs_time=s_time-xx
                 fe_time=e_time-xx
                 times = np.c_[fs_time,fe_time]

                 i = 0;
                 j = 750;

                 ann_row=s_time.size
                 for y in range(n-1):
                     if y == 0:
                         example = new_data_frame[0: 750]

                     elif j < row:
                         example = new_data_frame[i: j]

                     elif y==n-2:
                         example = new_data_frame[-750: ]

                     example = example.to_numpy()
                     match = False
                     for t in times:
                         if overlap([i/fs,j/fs],t):
                             match = True
                             break
                     if (match and not typee) or (not match and typee):
                         if first_d:
                             print("first d")
                             eeg_data = example
                             first_d = False
                         else:
                             eeg_data = np.dstack((eeg_data,example))

                     i = int(j - (fs / 5))
                     j = int(j + 750 - (fs /5))

            else:
                i = 0;
                j = 750;
                for y in range(n-1):
                    if y == 0:
                        example = new_data_frame[0: 750]
                    elif j < row:

                        example = new_data_frame[i: j]
                    elif y==n-2:
                        example = new_data_frame[-750: ]
                    example = example.to_numpy()
                    if first_d:
                        print("first d")
                        eeg_data = example
                        first_d = False
                    else:

                        eeg_data = np.dstack((eeg_data,example))

    return eeg_data.astype('float32')


normal_eval = my_read("normal/train/", "Annotations",1)
abnormal_eval = my_read("abnormal/train/", "Annotations",0)

normal_eval_dim = normal_eval.shape[-1]
normal_eval_zeros = np.zeros(normal_eval_dim)

abnormal_eval_dim = abnormal_eval.shape[-1]
abnormal_eval_ones = np.ones(abnormal_eval_dim)

eval_data = np.dstack((normal_eval, abnormal_eval))
eval_label = np.append(normal_eval_zeros, abnormal_eval_ones)

eval_data = np.swapaxes(eval_data,0,2)
bs,t,f = eval_data.shape

enc_labels = to_categorical(eval_label, num_classes=2)
eval_label= enc_labels





# ----------------------CHRONONET Testing-----------------------
from tensorflow.keras.layers import Input,Dense,concatenate,Flatten,GRU,Conv1D
from tensorflow.keras.models import Model
inputsin= Input(shape=(t,f))
# ------------------First Inception
tower1 = Conv1D(32, 2, strides=2,activation='relu',padding="causal")(inputsin)
tower1 = BatchNormalization()(tower1)
tower2 = Conv1D(32, 4, strides=2,activation='relu',padding="causal")(inputsin)
tower2 = BatchNormalization()(tower2)
tower3 = Conv1D(32, 8, strides=2,activation='relu',padding="causal")(inputsin)
tower3 = BatchNormalization()(tower3)
x = concatenate([tower1,tower2,tower3],axis=2)
x = Dropout(0.65)(x)

# ----------------------Second Inception
tower1 = Conv1D(32, 2, strides=2,activation='relu',padding="causal")(x)
tower1 = BatchNormalization()(tower1)
tower2 = Conv1D(32, 4, strides=2,activation='relu',padding="causal")(x)
tower2 = BatchNormalization()(tower2)
tower3 = Conv1D(32, 8, strides=2,activation='relu',padding="causal")(x)
tower3 = BatchNormalization()(tower3)
x = concatenate([tower1,tower2,tower3],axis=2)
x = Dropout(0.65)(x)

# ----------------------------------Third Inception
tower1 = Conv1D(32, 2, strides=2,activation='relu',padding="causal")(x)
tower1 = BatchNormalization()(tower1)
tower2 = Conv1D(32, 4, strides=2,activation='relu',padding="causal")(x)
tower2 = BatchNormalization()(tower2)
tower3 = Conv1D(32, 8, strides=2,activation='relu',padding="causal")(x)
tower3 = BatchNormalization()(tower3)
x = concatenate([tower1,tower2,tower3],axis=2)
x = Dropout(0.75)(x)

res1 = GRU(32,activation='tanh',return_sequences=True)(x)
res2 = GRU(32,activation='tanh',return_sequences=True)(res1)
res1_2 = concatenate([res1,res2],axis=2)
res3 = GRU(32,activation='tanh',return_sequences=True)(res1_2)
x = concatenate([res1,res2,res3])
x = GRU(32,activation='tanh')(x)

predictions = Dense(2,activation='softmax')(x)
model = Model(inputs=inputsin, outputs=predictions)

model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])


# early stopping
es = EarlyStopping(monitor='val_loss', min_delta=0.01, mode='min', verbose=1, patience=15)                          #patience
mc = ModelCheckpoint('model3flipped_acc.hdf5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)        #filepath (save model as)
mces = ModelCheckpoint('model3flipped_loss.hdf5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)      #filepath (save model as)

# fit model
csv_logger = CSVLogger('log.csv', append=True, separator=';') # csv logger

XTraining, XValidation, YTraining, YValidation = train_test_split(eval_data,eval_label,stratify=eval_label,test_size=0.1)
hist = model.fit(XTraining,YTraining,batch_size=4,epochs=250,validation_data=(XValidation,YValidation),callbacks=[mc,mces,csv_logger],shuffle=False)
#hist=model.fit(eval_data,eval_label,validation_split=0.1,epochs=500,batch_size=4,verbose=1,callbacks=[ mc,mces,csv_logger],shuffle=False) #epochs #split #

model.save("final_model")
model.save_weights('final_model_weights')

print('Dimensions of normal matrix and abnormal matrix are')
print(normal_eval.shape)
print(abnormal_eval.shape)
print('The End')


acc = hist.history['accuracy']
val_acc = hist.history['val_accuracy']
loss = hist.history['loss']
val_loss = hist.history['val_loss']

epochs = range(len(acc))




plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.show()

plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()
